// AI ë¬¸ì œ ìƒì„± ìœ í‹¸ë¦¬í‹°
import type { Problem } from '../types/index.ts';

// ëª¨ë¸ fallback ì„¤ì • (ì‚¬ìš©ì ìš”ì²­ ìˆœì„œë¡œ ì •ë ¬)
const modelConfigs = [
  {
    name: 'gemini-2.5-flash-lite',
    endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent',
    config: {
      temperature: 0.3,
      topK: 40,
      topP: 0.95,
      maxOutputTokens: 512
    }
  },
  {
    name: 'gemini-1.5-flash',
    endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent',
    config: {
      temperature: 0.3,
      topK: 40,
      topP: 0.95,
      maxOutputTokens: 512
    }
  },
  {
    name: 'gemini-2.0-flash',
    endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent',
    config: {
      temperature: 0.3,
      topK: 40,
      topP: 0.95,
      maxOutputTokens: 512
    }
  },
  {
    name: 'gpt-4o-mini',
    endpoint: 'https://api.openai.com/v1/chat/completions',
    config: {
      temperature: 0.3,
      max_tokens: 512
    }
  },
  {
    name: 'gpt-3.5-turbo-0125',
    endpoint: 'https://api.openai.com/v1/chat/completions',
    config: {
      temperature: 0.3,
      max_tokens: 512
    }
  },
  {
    name: 'gpt-4.1-mini',
    endpoint: 'https://api.openai.com/v1/chat/completions',
    config: {
      temperature: 0.3,
      max_tokens: 512
    }
  }
];

// Gemini API í˜¸ì¶œ í•¨ìˆ˜
const callGeminiAPI = async (modelConfig: typeof modelConfigs[0], prompt: string, apiKey: string) => {
  console.log(`ğŸ”„ ${modelConfig.name} ëª¨ë¸ë¡œ ì‹œë„ ì¤‘...`);
  
  const response = await fetch(`${modelConfig.endpoint}?key=${apiKey}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [{
        parts: [{
          text: prompt
        }]
      }],
      generationConfig: modelConfig.config
    })
  });

  if (!response.ok) {
    const errorData = await response.json();
    const errorMessage = errorData.error?.message || response.statusText;
    
    // API limit ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
    const isLimitError = errorMessage.includes('quota') || 
                        errorMessage.includes('limit') || 
                        errorMessage.includes('rate') ||
                        errorMessage.includes('overloaded') ||
                        errorMessage.includes('unavailable') ||
                        response.status === 429 ||
                        response.status === 403 ||
                        response.status === 503;
    
    if (isLimitError) {
      console.log(`âš ï¸ ${modelConfig.name} ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ (ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì „í™˜): ${errorMessage}`);
      throw new Error(`LIMIT_ERROR: ${errorMessage}`);
    } else {
      console.log(`âŒ ${modelConfig.name} ëª¨ë¸ ì—ëŸ¬: ${errorMessage}`);
      throw new Error(`API_ERROR: ${errorMessage}`);
    }
  }

  const data = await response.json();
  console.log(`âœ… ${modelConfig.name} ëª¨ë¸ ì„±ê³µ!`);
  
  if (!data.candidates || !data.candidates[0]) {
    throw new Error('API ì‘ë‹µì— candidatesê°€ ì—†ìŠµë‹ˆë‹¤.');
  }

  return data;
};

// GPT API í˜¸ì¶œ í•¨ìˆ˜
const callGPTAPI = async (modelConfig: typeof modelConfigs[0], prompt: string, apiKey: string) => {
  console.log(`ğŸ”„ ${modelConfig.name} ëª¨ë¸ë¡œ ì‹œë„ ì¤‘...`);
  
  const response = await fetch(modelConfig.endpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: modelConfig.name,
      messages: [{
        role: 'user',
        content: prompt
      }],
      temperature: modelConfig.config.temperature,
      max_tokens: modelConfig.config.max_tokens
    })
  });

  if (!response.ok) {
    const errorData = await response.json();
    const errorMessage = errorData.error?.message || response.statusText;
    
    // API limit ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
    const isLimitError = errorMessage.includes('quota') || 
                        errorMessage.includes('limit') || 
                        errorMessage.includes('rate') ||
                        errorMessage.includes('overloaded') ||
                        errorMessage.includes('unavailable') ||
                        response.status === 429 ||
                        response.status === 403 ||
                        response.status === 503;
    
    if (isLimitError) {
      console.log(`âš ï¸ ${modelConfig.name} ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ (ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì „í™˜): ${errorMessage}`);
      throw new Error(`LIMIT_ERROR: ${errorMessage}`);
    } else {
      console.log(`âŒ ${modelConfig.name} ëª¨ë¸ ì—ëŸ¬: ${errorMessage}`);
      throw new Error(`API_ERROR: ${errorMessage}`);
    }
  }

  const data = await response.json();
  console.log(`âœ… ${modelConfig.name} ëª¨ë¸ ì„±ê³µ!`);
  
  if (!data.choices || !data.choices[0]) {
    throw new Error('API ì‘ë‹µì— choicesê°€ ì—†ìŠµë‹ˆë‹¤.');
  }

  return data;
};

// ë²ˆì—­ ë¬¸ì œ ìƒì„± í•¨ìˆ˜
export const generateTranslationProblem = async (
  language: 'í•œêµ­ì–´' | 'ì¤‘êµ­ì–´',
  field: string,
  difficulty: 'ìƒ' | 'ì¤‘' | 'í•˜',
  customPrompt?: string
): Promise<Problem> => {
  const apiKey = import.meta.env.VITE_GEMINI_API_KEY;
  
  if (!apiKey) {
    throw new Error('Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
  }

  // í”„ë¡¬í”„íŠ¸ ìƒì„±
  const prompt = `Create a ${language} paragraph (3-4 sentences) about ${field} with ${difficulty} difficulty.

${customPrompt ? `Include: ${customPrompt}` : ''}

Return JSON:
{
  "korean": "í•œêµ­ì–´ ë¬¸ë‹¨",
  "chinese": "ì¤‘êµ­ì–´ ë²ˆì—­",
  "difficulty": "${difficulty}",
  "field": "${field}"
}`;

  // console.log('í”„ë¡¬í”„íŠ¸:', prompt); // ë¯¼ê° ì •ë³´ ìˆ¨ê¹€

  // ëª¨ë¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
  let lastError: Error | null = null;
  let data: any = null;
  let successfulModel: string | null = null;

  for (const modelConfig of modelConfigs) {
    try {
      // ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ API í˜¸ì¶œ
      if (modelConfig.name.startsWith('gemini')) {
        data = await callGeminiAPI(modelConfig, prompt, apiKey);
      } else if (modelConfig.name.startsWith('gpt')) {
        // GPT ëª¨ë¸ì˜ ê²½ìš° OpenAI API í‚¤ í•„ìš”
        const openaiApiKey = import.meta.env.VITE_OPENAI_API_KEY;
        if (!openaiApiKey) {
          console.log(`âš ï¸ ${modelConfig.name} ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ`);
          throw new Error(`LIMIT_ERROR: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`);
        }
        data = await callGPTAPI(modelConfig, prompt, openaiApiKey);
      }
      successfulModel = modelConfig.name;
      break; // ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
    } catch (error) {
      lastError = error as Error;
      
      // API limit ì—ëŸ¬ê°€ ì•„ë‹ˆë©´ ë‹¤ìŒ ëª¨ë¸ ì‹œë„í•˜ì§€ ì•ŠìŒ
      if (!lastError.message.startsWith('LIMIT_ERROR:')) {
        console.log(`âŒ ${modelConfig.name} ëª¨ë¸ì—ì„œ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ, ë‹¤ìŒ ëª¨ë¸ ì‹œë„ ì¤‘ë‹¨`);
        break;
      }
      
      console.log(`ğŸ”„ ${modelConfig.name} ëª¨ë¸ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ ì‹œë„...`);
    }
  }

  // ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
  if (!data) {
    const errorMessage = lastError?.message || 'ëª¨ë“  ëª¨ë¸ì—ì„œ API í˜¸ì¶œì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.';
    if (errorMessage.includes('overloaded') || errorMessage.includes('unavailable')) {
      throw new Error('í˜„ì¬ AI ì„œë²„ê°€ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    }
    throw lastError || new Error('ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
  }

  console.log(`âœ… ${successfulModel} ëª¨ë¸ë¡œ ë¬¸ì œ ìƒì„± ì„±ê³µ!`);

  let generatedText: string;
  
  if (successfulModel?.startsWith('gemini')) {
    // Gemini ì‘ë‹µ ì²˜ë¦¬
    const candidate = data.candidates[0];
    if (!candidate.content || !candidate.content.parts || !candidate.content.parts[0]) {
      throw new Error('API ì‘ë‹µ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.');
    }
    generatedText = candidate.content.parts[0].text;
  } else if (successfulModel?.startsWith('gpt')) {
    // GPT ì‘ë‹µ ì²˜ë¦¬
    const choice = data.choices[0];
    if (!choice.message || !choice.message.content) {
      throw new Error('API ì‘ë‹µ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.');
    }
    generatedText = choice.message.content;
  } else {
    throw new Error('ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ì‘ë‹µ í˜•ì‹ì…ë‹ˆë‹¤.');
  }
  // console.log('ìƒì„±ëœ í…ìŠ¤íŠ¸:', generatedText); // ë¯¼ê° ì •ë³´ ìˆ¨ê¹€
  
  if (!generatedText) {
    throw new Error('API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
  }

  // JSON íŒŒì‹± ì‹œë„
  try {
    // JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
    const jsonMatch = generatedText.match(/```json\s*([\s\S]*?)\s*```/) || 
                     generatedText.match(/\{[\s\S]*\}/);
    
    const jsonText = jsonMatch ? (jsonMatch[1] || jsonMatch[0]) : generatedText;
    const problemData = JSON.parse(jsonText);
    
    // ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
    const problem: Problem = {
      id: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      korean: problemData.korean || problemData.original || '',
      chinese: problemData.chinese || problemData.translation || '',
      ChatGPT_ë²ˆì—­: problemData.chinese || problemData.translation || '',
      difficulty: problemData.difficulty || difficulty,
      field: problemData.field || field
    };

    // í•„ìˆ˜ í•„ë“œ ê²€ì¦
    if (!problem.korean || !problem.chinese) {
      throw new Error('ìƒì„±ëœ ë¬¸ì œì— í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.');
    }

    return problem;
  } catch (parseError) {
    console.error('JSON íŒŒì‹± ì‹¤íŒ¨:', parseError);
    // console.log('ì›ë³¸ í…ìŠ¤íŠ¸:', generatedText); // ë¯¼ê° ì •ë³´ ìˆ¨ê¹€
    
    // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë¬¸ì œ ìƒì„±
    const lines = generatedText.split('\n').filter((line: string) => line.trim());
    const koreanLine = lines.find((line: string) => /[ê°€-í£]/.test(line)) || '';
    const chineseLine = lines.find((line: string) => /[\u4e00-\u9fff]/.test(line)) || '';
    
    if (!koreanLine && !chineseLine) {
      throw new Error('ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ í•œêµ­ì–´ë‚˜ ì¤‘êµ­ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }

    return {
      id: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      korean: koreanLine || 'ìƒ˜í”Œ í•œêµ­ì–´ ë¬¸ì¥',
      chinese: chineseLine || 'ìƒ˜í”Œ ì¤‘êµ­ì–´ ë¬¸ì¥',
      ChatGPT_ë²ˆì—­: chineseLine || 'ìƒ˜í”Œ ì¤‘êµ­ì–´ ë¬¸ì¥',
      difficulty,
      field
    };
  }
};

// ì—¬ëŸ¬ ë¬¸ì œ ì¼ê´„ ìƒì„±
export const generateMultipleProblems = async (
  count: number,
  language: 'í•œêµ­ì–´' | 'ì¤‘êµ­ì–´',
  field: string,
  difficulty: 'ìƒ' | 'ì¤‘' | 'í•˜',
  customPrompt?: string
): Promise<Problem[]> => {
  const problems: Problem[] = [];
  
  for (let i = 0; i < count; i++) {
    try {
      const problem = await generateTranslationProblem(language, field, difficulty, customPrompt);
      problems.push(problem);
      
      // API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ (ê³¼ë¶€í•˜ ë°©ì§€)
      if (i < count - 1) {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    } catch (error) {
      console.error(`ë¬¸ì œ ${i + 1} ìƒì„± ì‹¤íŒ¨:`, error);
      // ì‹¤íŒ¨í•œ ë¬¸ì œëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
    }
  }
  
  return problems;
};
